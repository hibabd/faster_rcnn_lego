# -*- coding: utf-8 -*-
"""finetuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RAGER9dCZ95hwOMHW_CQXr3APKGn9IB1

"""
# Setup
import pandas as pd
import os
import numpy as np
import torch, torchvision
import torch.utils.data
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from engine import train_one_epoch, evaluate
import utils
import transforms as T
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import math
import sys
import time
import torchvision.models.detection.mask_rcnn
from coco_utils import get_coco_api_from_dataset
from coco_eval import CocoEvaluator
from tabulate import tabulate

root = 'Real'
sortedBricks = [300121, 300123, 300124, 300321, 300323, 300401, 300421, 300423, 300424, 300821, 306201, 306223, 365901,
                614321, 623224, 654126, 4109995, 4114319, 4118793, 4121739, 4121967, 4143562, 4153825, 4153827, 4157124,
                4160403, 4165967,
                4181134, 4181135, 4181144, 4210718, 4211054, 4211149, 4211166, 4211201, 4211221, 4211412, 4211437,
                4211511, 4211614,
                4212454, 4504369, 4504381, 4517992, 4520632, 4527943, 4529240, 4529247, 4539099, 4548180, 4550325,
                4550348, 4558957,
                4567338, 4621545, 4639693, 4647553, 4649749, 4650630, 4651236, 4655172, 6003007, 6004938, 6022023,
                6022064, 6025026,
                6028324, 6035291, 6062574, 6069259, 6073026, 6074890, 6075075, 6075079, 6104154, 6138173, 6151663,
                6177697, 6223631,
                6225539, 6229071, 6238674, 6244886, 6249125, 6252809]
data_size = 3062
train_size = int(0.8 * data_size)
test_size = data_size - train_size
print("Data size: ", str(data_size), "Train size: ", str(train_size), "Test size: ", str(test_size))
num_classes = 86
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')


# Define dataset
class PennFudanDataset(torch.utils.data.Dataset):
    def __init__(self, root, transforms=None):
        self.root = root
        self.transforms = transforms
        self.imgs = list(sorted(os.listdir(os.path.join(root, "CroppedRenders"))))
        print(len(self.imgs) - 1)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root, "CroppedRenders/real_" + str(idx) + ".png")
        img = Image.open(img_path).convert("RGB")

        annotation = pd.read_csv(os.path.join(self.root, 'CroppedAnnotations/real_' + str(idx) + '.csv'))
        if not annotation[annotation.iloc[:, 1].isnull()].empty:
            print(annotation[annotation.iloc[:, 1].isnull()])
            # annotation.iloc[:, 1] = annotation.iloc[:, 1].astype(float)
            test = annotation.iloc[0][1]
            print(annotation)
            # print(" ")
            # print(test.dtype)
            # print(" ")
            # print(annotation.shape[0])
        # get bounding box coordinates for each object
        num_objs = annotation.shape[0]
        boxes = []
        labels = np.zeros(num_objs)
        # print(num_objs)
        for i in range(num_objs):
            # print(annotation.iloc[i][1].dtype)
            xmin = annotation.iloc[i][2]
            ymin = annotation.iloc[i][3]
            xmax = annotation.iloc[i][4]
            ymax = annotation.iloc[i][5]
            boxes.append(
                [np.minimum(xmin, xmax), np.minimum(ymin, ymax), np.maximum(xmin, xmax), np.maximum(ymin, ymax)])
            # labels[i] = sortedBricks.index(int(annotation.iloc[i][1])) + 1
            # labels[i] = find_label(str(int(annotation.iloc[i][1])))
        # convert everything into a torch.Tensor
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        # there is only one class
        labels = torch.as_tensor(labels, dtype=torch.int64)

        image_id = torch.tensor([idx])
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        # suppose all instances are not crowd
        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transforms is not None:
            img, target = self.transforms(img, target)

        return img, target

    def __len__(self):
        return len(self.imgs) - 1


# Define transform function for data processing

def get_transform(train):
    transforms = []
    # converts the image, a PIL image, into a PyTorch Tensor
    transforms.append(T.ToTensor())
    if train:
        # during training, randomly flip the training images
        # and ground-truth for data augmentation
        transforms.append(T.RandomHorizontalFlip(0.5))
    return T.Compose(transforms)


# Create custom train and test sets

# use our dataset and defined transformations
dataset = PennFudanDataset(root, get_transform(train=True))
dataset_test = PennFudanDataset(root, get_transform(train=False))

# split the dataset in train and test set
torch.manual_seed(1)
indices = torch.randperm(len(dataset)).tolist()
dataset = torch.utils.data.Subset(dataset, indices[:train_size])
dataset_test = torch.utils.data.Subset(dataset_test, indices[train_size:])

# define training and validation data loaders
data_loader = torch.utils.data.DataLoader(
    dataset, batch_size=10, shuffle=True, num_workers=2,
    collate_fn=utils.collate_fn)

data_loader_test = torch.utils.data.DataLoader(
    dataset_test, batch_size=10, shuffle=False, num_workers=2,
    collate_fn=utils.collate_fn)


# Create model and loss- and optimization functions

def get_instance_segmentation_model(num_classes):
    # load an instance segmentation model pre-trained on COCO
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
    # get the number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model


# load an instance segmentation model pre-trained on COCO
model = get_instance_segmentation_model(num_classes)
# move model to the right device
model.to(device)
# construct an optimizer
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005,
                            momentum=0.9, weight_decay=0.0005)

# and a learning rate scheduler which decreases the learning rate by
# 10x every 3 epochs
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                               step_size=3,
                                               gamma=0.1)


# Train and evaluate model

def train_one_epoch_test(model, optimizer, data_loader, device, epoch, print_freq):
    model.train()
    plot_losses = []
    metric_logger = utils.MetricLogger(delimiter="  ")
    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    header = 'Epoch: [{}]'.format(epoch)

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1. / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)

    for images, targets in metric_logger.log_every(data_loader, print_freq, header):
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)

        losses = sum(loss for loss in loss_dict.values())

        # reduce losses over all GPUs for logging purposes
        loss_dict_reduced = utils.reduce_dict(loss_dict)
        losses_reduced = sum(loss for loss in loss_dict_reduced.values())

        loss_value = losses_reduced.item()
        plot_losses.append(loss_value)

        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            print(loss_dict_reduced)
            sys.exit(1)

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)
        metric_logger.update(lr=optimizer.param_groups[0]["lr"])

    return sum(plot_losses)/len(plot_losses)


# let's train it for 10 epochs
num_epochs = 10
dict_losses = []

for epoch in range(num_epochs):
    dict_losses.append(train_one_epoch_test(model, optimizer, data_loader, device, epoch,print_freq=400))
    # update the learning rate
    lr_scheduler.step()
    # evaluate on the test dataset
    evaluate(model, data_loader_test, device=device)

## SAVE TRAINED MODEL
torch.save(model.state_dict(), 'faster-rcnn.pth')

with open('Faster-rcnn.txt', 'w+') as file:
    file.write(str(dict_losses))
    file.close()


## EVALUATE PERFORMANCE
def IoU(boxA, boxB):
    # determine the (x, y)-coordinates of the intersection rectangle
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    # compute the area of intersection rectangle
    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    # compute the area of both the prediction and ground-truth
    # rectangles
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)
    # compute the intersection over union by taking the intersection
    # area and dividing it by the sum of prediction + ground-truth
    # areas - the interesection area
    iou = interArea / float(boxAArea + boxBArea - interArea)
    # return the intersection over union value
    return iou


map = []
for i in range(len(dataset_test)):
    pr = dataset_test[i]
    map.append(pr[1]['image_id'].item())

print(map)


def evaluate_model(render):
    p = map.index(render)
    img, _ = dataset_test[p]
    groundtruth = pd.read_csv(root + '/CroppedAnnotations/real_' + str(render) + '.csv')

    # put the model in evaluation mode
    # model.eval()
    with torch.no_grad():
        prediction = model([img.to(device)])
        print(prediction)

    gts = []
    gt_labels = []
    for i in range(groundtruth.shape[0]):
        x1 = groundtruth.iloc[i][2]
        y1 = groundtruth.iloc[i][3]
        x2 = groundtruth.iloc[i][4]
        y2 = groundtruth.iloc[i][5]
        gts.append([x1, y1, x2, y2])
        gt_labels.append(sortedBricks.index(groundtruth.iloc[i][1]) + 1)

    prds = []
    pr_labels = []
    for i in range(len(prediction[0]['labels'])):
        if prediction[0]['scores'][i] > 0.5:
            x1 = prediction[0]['boxes'][i][0].item()
            y1 = prediction[0]['boxes'][i][1].item()
            x2 = prediction[0]['boxes'][i][2].item()
            y2 = prediction[0]['boxes'][i][3].item()
            prds.append([x1, y1, x2, y2])
            pr_labels.append(prediction[0]['labels'][i].item())

    # epsilon = 5
    tp = 0
    fp = 0
    fn = 0
    total = len(gts)
    recognized = []
    recognized_prds = []
    scores = []

    for i in range(len(gts)):
        j = 0
        while j < len(prds):
            gts_coords = gts[i]
            prds_coords = prds[j]
            # diff = abs(np.subtract(gts_coords, prds_coords))
            if IoU(gts_coords, prds_coords) > 0.5 and pr_labels[j] == gt_labels[i]:
                if gts_coords not in recognized and prds_coords not in recognized_prds:
                    tp += 1
                    recognized.append(gts_coords)
                    recognized_prds.append(prds_coords)
                    # prds.remove(prds_coords)
                    # gts.remove(gts_coords)
                    scores.append(prediction[0]['scores'][j].item())
            j += 1

    if len(gts) > len(recognized):
        # print(gts)
        fn = len(gts) - len(recognized)
    if len(prds) > len(recognized_prds):
        # print(prds)
        fp = len(prds) - len(recognized_prds)

    # print('Image: crender' + str(render) + '.png')
    # print('Total number of objects: ', total)
    # print('True Positives: ', str(tp))
    # print('False Negatives: ', str(fn))
    # print('False Positives: ', str(fp))
    return total, tp, fp, fn


total = 0
total_tp = 0
total_fp = 0
total_fn = 0

for i in range(len(map)):
    tot, tp, fp, fn = evaluate_model(map[i])
    total += tot
    total_tp += tp
    total_fp += fp
    total_fn += fn


def calculate_precision(tp, fp):
    return tp / (tp + fp)


def calculate_recall(tp, fn):
    return tp / (tp + fn)


def calculate_f1score(tp, fp, fn):
    pr = calculate_precision(tp, fp)
    re = calculate_recall(tp, fn)
    return (2 * pr * re) / (pr + re)


def calculate_quality(tp, fp, fn):
    return tp / (tp + fp + fn)


table = [["Trained on", train_size], ["Tested on", test_size], ["Objects", total], ["TP", total_tp], ["FN", total_fn],
         ["FP", total_fp],
         ["Precision", calculate_precision(total_tp, total_fp)], ["Recall", calculate_recall(total_tp, total_fn)],
         ["F1 Score", calculate_f1score(total_tp, total_fp, total_fn)],
         ["Quality", calculate_quality(total_tp, total_fp, total_fn)]]
print(tabulate(table, headers=["Results", "Synthetic"]))
print("Accuracy: ", str((total_tp / total) * 100), "%")
